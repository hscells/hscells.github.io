<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Harry Scells on Harry Scells</title>
    <link>/</link>
    <description>Recent content in Harry Scells on Harry Scells</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-au</language>
    <copyright>&amp;copy; 1995-2019</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +1000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Automatic Boolean Query Refinement for Systematic Review Literature Search</title>
      <link>/publication/www2019_boolean/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/www2019_boolean/</guid>
      <description>&lt;p&gt;In the medical domain, systematic reviews are a highly trustworthy evidence source used to inform clinical diagnosis and treatment, and governmental policy making. Systematic reviews must be complete in that &lt;em&gt;all relevant&lt;/em&gt; literature for the research question of the review must be synthesised in order to produce a recommendation. To identify the literature to screen for inclusion in systematic reviews, information specialists construct complex Boolean queries that capture the information needs defined by the research questions of the systemic review.
However, in the quest for total recall, these Boolean queries return many non relevant results.&lt;/p&gt;

&lt;p&gt;In this paper, we propose automatic methods for Boolean query refinement in the context of systematic review literature retrieval with the aim of alleviating this high recall-low precision problem. To do this, we build upon current literature and define additional semantic transformations for Boolean queries in the form of query expansion and reduction. Empirical evaluation is done on a set of real systematic review queries to show how our method performs in a realistic setting.
We found that query refinement strategies produced queries that were more effective than the original in terms of six information retrieval evaluation measures. In particular, queries were refined  to increase precision, while maintaining, or even increasing, recall &amp;mdash; this, in turn, translates into both time and cost savings when creating laborious and expensive systematic reviews.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Information Retrieval Experiment Framework for Domain Specific Applications</title>
      <link>/publication/sigir2018_framework/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/sigir2018_framework/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating Better Queries for Systematic Reviews</title>
      <link>/publication/sigir2018_generating/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/sigir2018_generating/</guid>
      <description>&lt;p&gt;Systematic reviews form the cornerstone of evidence based medicine, aiming to answer complex medical questions based on all evidence currently available. Key to the effectiveness of a systematic review is an (often large) Boolean query used to search large publication repositories. These Boolean queries are carefully crafted by researchers and information specialists, and often reviewed by a panel of experts. However, little is known about the effectiveness of the Boolean queries at the time of formulation.&lt;/p&gt;

&lt;p&gt;In this paper we investigate whether a better Boolean query than that defined in the protocol of a systematic review, can be created, and we develop methods for the transformation of a given Boolean query into a more effective one. Our approach involves defining possible transformations of Boolean queries and their clauses. It also involves casting the problem of identifying a transformed query that is better than the original into: (i) a classification problem; and (ii) a learning to rank problem. Empirical experiments are conducted on a real set of systematic reviews. Analysis of results shows that query transformations that are better than the original queries do exist, and that our approaches are able to select more effective queries from the set of possible transformed queries so as to maximise different target effectiveness measures.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Query Variation Performance Prediction for Systematic Reviews</title>
      <link>/publication/sigir2018_qvpp/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/sigir2018_qvpp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>searchrefiner: A Query Visualisation and Understanding Tool for Systematic Reviews</title>
      <link>/publication/cikm2018_searchrefiner/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/cikm2018_searchrefiner/</guid>
      <description>&lt;p&gt;We present an open source tool, searchrefiner, for researchers that conduct medical systematic reviews to assist in formulating, visualising, and understanding Boolean queries. The searchrefiner web interface allows researchers to explore how Boolean queries retrieve citations in existing, popular query syntaxes used in systematic review literature search. The web interface allows researchers to perform tasks such as using validation citations to ensure queries are retrieving a minimum set of known relevant citations, and editing Boolean queries by dragging and dropping clauses in a structured editor. In addition, the tools provided by the searchrefiner interface allow researchers to visualise why the queries they formulate retrieve citations, and ways to understand how to refine queries into more effective ones. searchrefiner is targeted at both experts and novices, as a tool for query formulation and refinement, and as a tool for training users to search for literature to compile systematic reviews.&lt;/p&gt;

&lt;p&gt;The searchrefiner website located at &lt;a href=&#34;https://ielab.io/searchrefiner&#34; target=&#34;_blank&#34;&gt;https://ielab.io/searchrefiner&lt;/a&gt; contains information about how to download, install, and use the tool, as well as a link to an online hosted version for demonstration purposes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Test Collection for Evaluating Retrieval of Studies for Inclusion in Systematic Reviews</title>
      <link>/publication/sigir2017_collection/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/sigir2017_collection/</guid>
      <description>&lt;p&gt;We introduce a test collection for evaluating the effectiveness of different methods used to retrieve research studies for inclusion in systematic reviews. Systematic reviews appraise and synthesise studies that meet specific inclusion criteria. Systematic reviews intended for a biomedical science audience use boolean queries with many, often complex, search clauses to retrieve studies; these are then manually screened to determine eligibility for inclusion in the review. This process is expensive and time consuming. The development of systems that improve retrieval effectiveness will have an immediate impact by reducing the complexity and resources required for this process. Our test collection consists of approximately 26 million research studies extracted from the freely available MEDLINE database, 94 review (query) topics extracted from Cochrane systematic reviews, and corresponding relevance assessments. Tasks for which the collection can be used for information retrieval system evaluation are described and the use of the collection to evaluate common baselines within one such task is demonstrated. See the links to this side of this page for links to the paper and to download the collection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Integrating the Framing of Clinical Questions via PICO into the Retrieval of Medical Literature for Systematic Reviews</title>
      <link>/publication/cikm2017_pico/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/cikm2017_pico/</guid>
      <description></description>
    </item>
    
    <item>
      <title>QUT ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning To Rank</title>
      <link>/publication/clef2017_tar/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/clef2017_tar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reducing Workload of Systematic Review Searching and Screening Processes</title>
      <link>/publication/fdia2017_essir/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/fdia2017_essir/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating Methods Of Annotating Lifelogs For Use In Search</title>
      <link>/publication/honours_thesis/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/publication/honours_thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>QUT at the NTCIR Lifelog Semantic Access Task</title>
      <link>/publication/ntcir2016_lifelog/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/publication/ntcir2016_lifelog/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
