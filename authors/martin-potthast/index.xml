<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Martin Potthast | Harry Scells</title><link>https://scells.me/authors/martin-potthast/</link><atom:link href="https://scells.me/authors/martin-potthast/index.xml" rel="self" type="application/rss+xml"/><description>Martin Potthast</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 08 Nov 2023 00:00:00 +0000</lastBuildDate><image><url>https://scells.me/media/icon_hu33ac2647be7de12c09d025d9374b4e08_2029_512x512_fill_lanczos_center_3.png</url><title>Martin Potthast</title><link>https://scells.me/authors/martin-potthast/</link></image><item><title>Evaluating Generative Ad Hoc Information Retrieval</title><link>https://scells.me/publication/arxiv2023_evaluating/</link><pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/arxiv2023_evaluating/</guid><description>&lt;p>Recent advances in large language models have enabled the development of viable generative information retrieval systems. A generative retrieval system returns a grounded generated text in response to an information need instead of the traditional document ranking. Quantifying the utility of these types of responses is essential for evaluating generative retrieval systems. As the established evaluation methodology for ranking-based ad hoc retrieval may seem unsuitable for generative retrieval, new approaches for reliable, repeatable, and reproducible experimentation are required. In this paper, we survey the relevant information retrieval and natural language processing literature, identify search tasks and system architectures in generative retrieval, develop a corresponding user model, and study its operationalization. This theoretical analysis provides a foundation and new insights for the evaluation of generative ad hoc retrieval systems.&lt;/p></description></item><item><title>Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation</title><link>https://scells.me/publication/sigirap2023_generating/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigirap2023_generating/</guid><description>&lt;p>Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title.&lt;/p></description></item><item><title>pybool_ir: A Toolkit for Domain-Specific Search Experiments</title><link>https://scells.me/publication/sigir2023_pyboolir/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_pyboolir/</guid><description>&lt;p>Undertaking research in domain-specific scenarios such as systematic review literature search, legal search, and patent search can often have a high barrier of entry due to complicated indexing procedures and complex Boolean query syntax. Indexing and searching document collections like PubMed in off-the-shelf tools such as Elasticsearch and Lucene often yields less accurate (and less effective) results than the PubMed search engine, i.e., retrieval results do not match what would be retrieved if one issued the same query to PubMed. Furthermore, off-the-shelf tools have their own nuanced query languages and do not allow directly using the often large and complicated Boolean queries seen in domain-specific search scenarios. The pybool_ir toolkit aims to address these problems and to lower the barrier to entry for developing new methods for domain-specific search. The toolkit is an open source package available at &lt;a href="https://github.com/hscells/pybool_ir" target="_blank" rel="noopener">https://github.com/hscells/pybool_ir&lt;/a>.&lt;/p></description></item><item><title>Smooth Operators for Effective Systematic Review Queries</title><link>https://scells.me/publication/sigir2023_smooth/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_smooth/</guid><description>&lt;p>Effective queries are crucial to minimising the time and cost of medical systematic reviews, as all retrieved documents must be judged for relevance. Boolean queries, developed by expert librarians, are the standard for systematic reviews. They guarantee reproducible and verifiable retrieval and more control than free-text queries. However, the result sets of Boolean queries are unranked and difficult to control due to the strict Boolean operators. We address these problems in a single unified retrieval model by formulating a class of smooth operators that are compatible with and extend existing Boolean operators. Our smooth operators overcome several shortcomings of previous extensions of the Boolean retrieval model. In particular, our operators are independent of the underlying ranking function, so that exact-match and large language model rankers can be combined in the same query. We found that replacing Boolean operators with equivalent or similar smooth operators often improves the effectiveness of queries. Their properties make tuning a query to precision or recall intuitive and allow greater control over how documents are retrieved. This additional control leads to more effective queries and reduces the cost of systematic reviews.&lt;/p></description></item><item><title>The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives</title><link>https://scells.me/publication/sigir2023_aql/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_aql/</guid><description>&lt;p>The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 137 million search result pages, and 1.4 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry.&lt;/p></description></item><item><title>Australiaâ€“Germany Joint Research Cooperation Scheme</title><link>https://scells.me/project/ua-daad-ppp-23/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://scells.me/project/ua-daad-ppp-23/</guid><description/></item></channel></rss>