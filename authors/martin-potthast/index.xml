<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Martin Potthast | Harry Scells</title><link>/authors/martin-potthast/</link><atom:link href="/authors/martin-potthast/index.xml" rel="self" type="application/rss+xml"/><description>Martin Potthast</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 11 Jul 2024 00:00:00 +0000</lastBuildDate><image><url>/media/icon_hu33ac2647be7de12c09d025d9374b4e08_2029_512x512_fill_lanczos_center_3.png</url><title>Martin Potthast</title><link>/authors/martin-potthast/</link></image><item><title>Evaluating Generative Ad Hoc Information Retrieval</title><link>/publication/sigir2024_evaluating/</link><pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate><guid>/publication/sigir2024_evaluating/</guid><description>&lt;p>Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the established evaluation methodology for ranking-based ad hoc retrieval is not suited for the reliable and reproducible evaluation of generated responses. To lay a foundation for developing new evaluation methods for generative retrieval systems, we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a new user model, and study its operationalization.&lt;/p></description></item><item><title>Resources for Combining Teaching and Research in Information Retrieval Coursework</title><link>/publication/sigir2024_teaching/</link><pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate><guid>/publication/sigir2024_teaching/</guid><description>&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Nominated for best resource paper award!&lt;/strong>
&lt;/div>
&lt;/div>
&lt;p>A recent study showed that students in IR courses are especially motivated and learn more effectively when they participate in shared tasks as part of their coursework. To support teachers in integrating such activities, we present Web IDE-based applications and tutorials that employ TIREx and ir_datasets to cover the process of a typical shared task in IR: from creating test collections over developing retrieval systems to making relevance judgments and finally statistically analyzing the results. Using our tools, students can gain hands-on experience with empirical IR research by working on some current shared task, by working on earlier collections, or by creating new ones. Our experiences in implementing the cor- responding teaching concept in four IR courses at two universities confirm that students are very motivated to conduct research, and we also find that some of the resulting artifacts (e.g., students’ test collections and retrieval approaches) are of really good quality.&lt;/p></description></item><item><title>Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders</title><link>/publication/arxiv2024_setencoder/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>/publication/arxiv2024_setencoder/</guid><description>&lt;p>Cross-encoders are effective passage re-rankers. But when re-ranking multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant. Moreover, their high memory footprint constrains the number of passages during listwise training. To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters. Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective.&lt;/p></description></item><item><title>Zero-shot Generative Large Language Models for Systematic Review Screening Automation</title><link>/publication/ecir2024_zero/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>/publication/ecir2024_zero/</guid><description>&lt;p>Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models (LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.&lt;/p></description></item><item><title>Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation</title><link>/publication/sigirap2023_generating/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>/publication/sigirap2023_generating/</guid><description>&lt;p>Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title.&lt;/p></description></item><item><title>pybool_ir: A Toolkit for Domain-Specific Search Experiments</title><link>/publication/sigir2023_pyboolir/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>/publication/sigir2023_pyboolir/</guid><description>&lt;p>Undertaking research in domain-specific scenarios such as systematic review literature search, legal search, and patent search can often have a high barrier of entry due to complicated indexing procedures and complex Boolean query syntax. Indexing and searching document collections like PubMed in off-the-shelf tools such as Elasticsearch and Lucene often yields less accurate (and less effective) results than the PubMed search engine, i.e., retrieval results do not match what would be retrieved if one issued the same query to PubMed. Furthermore, off-the-shelf tools have their own nuanced query languages and do not allow directly using the often large and complicated Boolean queries seen in domain-specific search scenarios. The pybool_ir toolkit aims to address these problems and to lower the barrier to entry for developing new methods for domain-specific search. The toolkit is an open source package available at &lt;a href="https://github.com/hscells/pybool_ir" target="_blank" rel="noopener">https://github.com/hscells/pybool_ir&lt;/a>.&lt;/p></description></item><item><title>Smooth Operators for Effective Systematic Review Queries</title><link>/publication/sigir2023_smooth/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>/publication/sigir2023_smooth/</guid><description>&lt;p>Effective queries are crucial to minimising the time and cost of medical systematic reviews, as all retrieved documents must be judged for relevance. Boolean queries, developed by expert librarians, are the standard for systematic reviews. They guarantee reproducible and verifiable retrieval and more control than free-text queries. However, the result sets of Boolean queries are unranked and difficult to control due to the strict Boolean operators. We address these problems in a single unified retrieval model by formulating a class of smooth operators that are compatible with and extend existing Boolean operators. Our smooth operators overcome several shortcomings of previous extensions of the Boolean retrieval model. In particular, our operators are independent of the underlying ranking function, so that exact-match and large language model rankers can be combined in the same query. We found that replacing Boolean operators with equivalent or similar smooth operators often improves the effectiveness of queries. Their properties make tuning a query to precision or recall intuitive and allow greater control over how documents are retrieved. This additional control leads to more effective queries and reduces the cost of systematic reviews.&lt;/p></description></item><item><title>The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives</title><link>/publication/sigir2023_aql/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>/publication/sigir2023_aql/</guid><description>&lt;p>The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 137 million search result pages, and 1.4 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry.&lt;/p></description></item><item><title>Australia–Germany Joint Research Cooperation Scheme</title><link>/project/ua-daad-ppp-23/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>/project/ua-daad-ppp-23/</guid><description/></item></channel></rss>