<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Harrisen Scells | Harry Scells</title><link>https://scells.me/authors/harrisen-scells/</link><atom:link href="https://scells.me/authors/harrisen-scells/index.xml" rel="self" type="application/rss+xml"/><description>Harrisen Scells</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Feb 2024 00:00:00 +0000</lastBuildDate><image><url>https://scells.me/media/icon_hu33ac2647be7de12c09d025d9374b4e08_2029_512x512_fill_lanczos_center_3.png</url><title>Harrisen Scells</title><link>https://scells.me/authors/harrisen-scells/</link></image><item><title>Zero-shot Generative Large Language Models for Systematic Review Screening Automation</title><link>https://scells.me/publication/ecir2024_zero/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ecir2024_zero/</guid><description>&lt;p>Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models (LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.&lt;/p></description></item><item><title>Evaluating Generative Ad Hoc Information Retrieval</title><link>https://scells.me/publication/arxiv2023_evaluating/</link><pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/arxiv2023_evaluating/</guid><description>&lt;p>Recent advances in large language models have enabled the development of viable generative information retrieval systems. A generative retrieval system returns a grounded generated text in response to an information need instead of the traditional document ranking. Quantifying the utility of these types of responses is essential for evaluating generative retrieval systems. As the established evaluation methodology for ranking-based ad hoc retrieval may seem unsuitable for generative retrieval, new approaches for reliable, repeatable, and reproducible experimentation are required. In this paper, we survey the relevant information retrieval and natural language processing literature, identify search tasks and system architectures in generative retrieval, develop a corresponding user model, and study its operationalization. This theoretical analysis provides a foundation and new insights for the evaluation of generative ad hoc retrieval systems.&lt;/p></description></item><item><title>Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation</title><link>https://scells.me/publication/sigirap2023_generating/</link><pubDate>Mon, 09 Oct 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigirap2023_generating/</guid><description>&lt;p>Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title.&lt;/p></description></item><item><title>Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of Information Retrieval Models</title><link>https://scells.me/publication/ictir2023_c02/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ictir2023_c02/</guid><description>&lt;p>As in other fields of artificial intelligence, the information retrieval community has grown interested in investigating the power consumption associated with neural models, particularly models of search. This interest has become particularly relevant as the energy consumption of information retrieval models has risen with new neural models based on large language models, leading to an associated increase of CO2 emissions, albeit relatively low compared to fields such as natural language processing. Consequently, researchers have started exploring the development of a green agenda for sustainable information retrieval research and operation. Previous work, however, has primarily considered energy consumption and associated CO2 emissions alone. In this paper, we seek to draw the information retrieval community&amp;rsquo;s attention to the overlooked aspect of water consumption related to these powerful models. We supplement previous energy consumption estimates with corresponding water consumption estimates, considering both off-site water consumption (required for operating and cooling energy production systems such as carbon and nuclear power plants) and on-site consumption (for cooling the data centres where models are trained and operated). By incorporating water consumption alongside energy consumption and CO2 emissions, we offer a more comprehensive understanding of the environmental impact of information retrieval research and operation.&lt;/p></description></item><item><title>Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search</title><link>https://scells.me/publication/adcs2022_neural/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/adcs2022_neural/</guid><description>&lt;p>Medical systematic reviews typically require assessing all the documents retrieved by a search. The reason is two-fold: the task aims for “total recall”; and documents retrieved using Boolean search are an unordered set, and thus it is unclear how an assessor could examine only a subset. Screening prioritisation is the process of ranking the (unordered) set of retrieved documents, allowing assessors to begin the downstream processes of the systematic review creation earlier, leading to earlier completion of the review, or even avoiding screening documents ranked least relevant.&lt;/p>
&lt;p>Screening prioritisation requires highly effective ranking methods. Pre-trained language models are state-of-the-art on many IR tasks but have yet to be applied to systematic review screening prioritisation. In this paper, we apply several pre-trained language models to the systematic review document ranking task, both directly and fine-tuned. An empirical analysis compares how effective neural methods compare to traditional methods for this task. We also investigate different types of document representations for neural methods and their impact on ranking performance.&lt;/p>
&lt;p>Our results show that BERT-based rankers outperform the current state-of-the-art screening prioritisation methods. However, BERT rankers and existing methods can actually be complementary, and thus, further improvements may be achieved if used in conjunction.&lt;/p></description></item><item><title>Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search?</title><link>https://scells.me/publication/sigir2023_chatgpt/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_chatgpt/</guid><description>&lt;p>Systematic reviews are comprehensive literature reviews for a highly focused research question. These reviews are considered the highest form of evidence in medicine. Complex Boolean queries are developed as part of the systematic review creation process to retrieve literature, as they permit reproducibility and understandability. However, it is difficult and time-consuming to develop high-quality Boolean queries, often requiring the expertise of expert searchers like librarians. Recent advances in transformer-based generative models have shown their ability to effectively follow user instructions and generate answers based on these instructions. In this paper, we investigate ChatGPT as a means for automatically formulating and refining complex Boolean queries for systematic review literature search. Overall, our research finds that ChatGPT has the potential to generate effective Boolean queries. The ability of ChatGPT to follow complex instructions and generate highly precise queries makes it a tool of potential value for researchers conducting systematic reviews, particularly for rapid reviews where time is a constraint and where one can trade off higher precision for lower recall. We also identify several caveats in using ChatGPT for this task, highlighting that this technology needs further validation before it is suitable for widespread uptake.&lt;/p></description></item><item><title>pybool_ir: A Toolkit for Domain-Specific Search Experiments</title><link>https://scells.me/publication/sigir2023_pyboolir/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_pyboolir/</guid><description>&lt;p>Undertaking research in domain-specific scenarios such as systematic review literature search, legal search, and patent search can often have a high barrier of entry due to complicated indexing procedures and complex Boolean query syntax. Indexing and searching document collections like PubMed in off-the-shelf tools such as Elasticsearch and Lucene often yields less accurate (and less effective) results than the PubMed search engine, i.e., retrieval results do not match what would be retrieved if one issued the same query to PubMed. Furthermore, off-the-shelf tools have their own nuanced query languages and do not allow directly using the often large and complicated Boolean queries seen in domain-specific search scenarios. The pybool_ir toolkit aims to address these problems and to lower the barrier to entry for developing new methods for domain-specific search. The toolkit is an open source package available at &lt;a href="https://github.com/hscells/pybool_ir" target="_blank" rel="noopener">https://github.com/hscells/pybool_ir&lt;/a>.&lt;/p></description></item><item><title>Smooth Operators for Effective Systematic Review Queries</title><link>https://scells.me/publication/sigir2023_smooth/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_smooth/</guid><description>&lt;p>Effective queries are crucial to minimising the time and cost of medical systematic reviews, as all retrieved documents must be judged for relevance. Boolean queries, developed by expert librarians, are the standard for systematic reviews. They guarantee reproducible and verifiable retrieval and more control than free-text queries. However, the result sets of Boolean queries are unranked and difficult to control due to the strict Boolean operators. We address these problems in a single unified retrieval model by formulating a class of smooth operators that are compatible with and extend existing Boolean operators. Our smooth operators overcome several shortcomings of previous extensions of the Boolean retrieval model. In particular, our operators are independent of the underlying ranking function, so that exact-match and large language model rankers can be combined in the same query. We found that replacing Boolean operators with equivalent or similar smooth operators often improves the effectiveness of queries. Their properties make tuning a query to precision or recall intuitive and allow greater control over how documents are retrieved. This additional control leads to more effective queries and reduces the cost of systematic reviews.&lt;/p></description></item><item><title>The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives</title><link>https://scells.me/publication/sigir2023_aql/</link><pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2023_aql/</guid><description>&lt;p>The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 137 million search result pages, and 1.4 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry.&lt;/p></description></item><item><title>Australia–Germany Joint Research Cooperation Scheme</title><link>https://scells.me/project/ua-daad-ppp-23/</link><pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate><guid>https://scells.me/project/ua-daad-ppp-23/</guid><description/></item><item><title>Alexander von Humboldt Research Fellowship</title><link>https://scells.me/project/alive/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://scells.me/project/alive/</guid><description>&lt;p>How do clinicians become informed to treat their patients? How do governments and institutions make health policy decisions?&lt;/p>
&lt;p>At heart, the answer to both of these questions is consulting a specific, focused, and comprehensive systematic review of medical literature. In the medical domain, systematic reviews are central to decision-making for both clinical practice (e.g., &amp;ldquo;Does hydrocloroquin treat COVID-19?&amp;rdquo;), as well as institutional and governmental medical policy-making (e.g., &amp;ldquo;Should everyone wear a mask during the COVID-19 pandemic?&amp;rdquo;). Once a review is published. However, new studies and evidence can arise or be retracted, which may change the review&amp;rsquo;s conclusions. In areas of medicine where there is a priority for decision-making, a level of uncertainty in existing studies, and the frequency of new medical studies is high (e.g., identifying the most effective treatment for COVID-19), systematic reviews are infeasible.&lt;/p>
&lt;p>This infeasibility is primarily due to the lengthy and costly processes involved in creating these systematic reviews. The most expensive and time-intensive aspect is the manual, critical appraisal of studies, which can take several years and cost upwards of 220,000 Euros to complete. Already, several researchers have highlighted that systematic reviews are not sufficient for the pace at which studies are currently being published. Several studies have spawned to address this problem, such as machine learning methods, summarisation of studies, guidelines for building automation tools, and even proposing a new type of &amp;ldquo;living&amp;rdquo; systematic review. However, I will be taking a radically different approach to answer the initial questions posed above in this project. I plan to focus on synthesising direct answers for highly focused, complex medical questions using NLP methods.&lt;/p>
&lt;p>To this end, I will investigate, apply, and derive new state-of-the-art methods for direct answer synthesis within the medical domain.&lt;/p></description></item><item><title>Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search</title><link>https://scells.me/publication/iswa2022_automated/</link><pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/iswa2022_automated/</guid><description>&lt;p>Medical systematic review query formulation is a highly complex task done by trained information specialists. Complexity comes from the reliance on lengthy Boolean queries, which express a detailed research question. To aid query formulation, information specialists use a set of exemplar documents, called ‘seed studies’, prior to query formulation. Seed studies help verify the effectiveness of a query prior to the full assessment of retrieved studies. Beyond this use of seeds, specific IR methods can exploit seed studies for guiding both automatic query formulation and new retrieval models. One major limitation of work to date is that these methods exploit `pseudo seed studies’ through retrospective use of included studies (i.e., relevance assessments). However, we show pseudo seed studies are not representative of real seed studies used by information specialists. Hence, we provide a test collection with real world seed studies used to assist with the formulation of queries. To support our collection, we provide an analysis, previously not possible, on how seed studies impact retrieval and perform several experiments using seed-study based methods to compare the effectiveness of using seed studies versus pseudo seed studies. We make our test collection and the results of all of our experiments and analysis available at &lt;a href="http://github.com/ielab/sysrev-seed-collection" target="_blank" rel="noopener">http://github.com/ielab/sysrev-seed-collection&lt;/a>. High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this context are highly complex, based on Boolean logic, include free-text terms and index terms from standardised terminologies (e.g., the Medical Subject Headings (MeSH) thesaurus), and are difficult and time-consuming to build. The use of MeSH terms, in particular, has been shown to improve the quality of the search results. However, identifying the correct MeSH terms to include in a query is difficult: information experts are often unfamiliar with the MeSH database and unsure about the appropriateness of MeSH terms for a query. Naturally, the full value of the MeSH terminology is often not fully exploited. This article investigates methods to suggest MeSH terms based on an initial Boolean query that includes only free-text terms. In this context, we devise lexical and pre-trained language models based methods. These methods promise to automatically identify highly effective MeSH terms for inclusion in a systematic review query. Our study contributes an empirical evaluation of several MeSH term suggestion methods. We further contribute an extensive analysis of MeSH term suggestions for each method and how these suggestions impact the effectiveness of Boolean queries.&lt;/p></description></item><item><title>Reduce, Reuse, Recycle: Green Information Retrieval Research</title><link>https://scells.me/publication/sigir2022_greenir/</link><pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2022_greenir/</guid><description>&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Awarded Best Paper Honorable Mention!&lt;/strong>
&lt;/div>
&lt;/div>
&lt;p>Recent advances in Information Retrieval utilise energy-intensive hardware to produce state-of-the-art results. In areas of research highly related to Information Retrieval, such as Natural Language Processing and Machine Learning, there have been efforts to quantify and reduce the power and emissions produced by methods that depend on such hardware. Research that is conscious of the environmental impacts of its experimentation and takes steps to mitigate some of these impacts is considered &amp;lsquo;Green&amp;rsquo;. Given the continuous demand for more data and power-hungry techniques, Green research is likely to become more important within the broader research community. Therefore, within the Information Retrieval community, the consequences of non-Green (in other words, Red) research should at least be considered and acknowledged. As such, the aims of this perspective paper are fourfold: (1) to review the Green literature not only for Information Retrieval but also for related domains in order to identify transferable Green techniques; (2) to provide measures for quantifying the power usage and emissions of Information Retrieval research; (3) to report the power usage and emission impacts for various current IR methods; and (4) to provide a framework to guide Green Information Retrieval research, taking inspiration from &amp;lsquo;reduce, reuse, recycle&amp;rsquo; waste management campaigns, including salient examples from the literature that implement these concepts.&lt;/p></description></item><item><title>From Little Things Big Things Grow: A Collection with Seed Studies for Medical Systematic Review Literature Search</title><link>https://scells.me/publication/sigir2022_seed/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2022_seed/</guid><description>&lt;p>Fine-grained logging of interactions in user studies is important for studying user behaviour, among other reasons. However, in many research scenarios, the way interactions are logged are usually tied to a monolithic system. We present a generic, application-independent service for logging interactions in web-pages, specifically targetting user studies. Our service, Big Brother, can be dropped-in to existing user interfaces with almost no configuration required by researchers. Big Brother has already been used in several user studies to record interactions in a number of user study research scenarios, such as lab-based and crowdsourcing environments. We further demonstrate the ability for Big Brother to scale to very large user studies through benchmarking experiments. Big Brother also provides a number of additional tools for visualising and analysing interactions.&lt;/p>
&lt;p>Big Brother significantly lowers the barrier to entry for logging user interactions by providing a minimal but powerful, no configuration necessary, service for researchers and practitioners of user studies that can scale to thousands of concurrent sessions. We have made the source code and releases for Big Brother available for download at &lt;a href="https://github.com/hscells/bigbro" target="_blank" rel="noopener">https://github.com/hscells/bigbro&lt;/a>.&lt;/p></description></item><item><title>MeSH Term Suggestion for Systematic Review Literature Search</title><link>https://scells.me/publication/adcs2021_mesh/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/adcs2021_mesh/</guid><description>&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Awarded Best Student Paper!&lt;/strong>
&lt;/div>
&lt;/div>
&lt;p>High-quality medical systematic reviews require comprehensive literature searches to ensure the recommendations and outcomes are sufficiently reliable. Indeed, searching for relevant medical literature is a key phase in constructing systematic reviews and often involves domain (medical researchers) and search (information specialists) experts in developing the search queries. Queries in this context are highly complex, based on Boolean logic, include free-text terms and index terms from standardised terminologies (e.g., MeSH), and are difficult and time-consuming to build. The use of MeSH terms, in particular, has been shown to improve the quality of the search results. However, identifying the correct MeSH terms to include in a query is difficult: information experts are often unfamiliar with the MeSH database and unsure about the appropriateness of MeSH terms for a query. Naturally, the full value of the MeSH terminology is often not fully exploited.&lt;/p>
&lt;p>This paper investigates methods to suggest MeSH terms based on an initial Boolean query that includes only free-text terms. These methods promise to automatically identify highly effective MeSH terms for inclusion in a systematic review query. Our study contributes an empirical evaluation of several MeSH term suggestion methods. We perform an extensive analysis of the retrieval, ranking, and refinement of MeSH term suggestions for each method and how these suggestions impact the effectiveness of Boolean queries.&lt;/p></description></item><item><title>SDR for Systematic Reviews: A Reproducibility Study</title><link>https://scells.me/publication/ecir2022_sdr/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ecir2022_sdr/</guid><description>&lt;p>Screening or assessing studies is critical to the quality and outcomes of a systematic review. Typically, a Boolean query retrieves the set of studies to screen. As the set of studies retrieved is unordered, screening all retrieved studies is usually required for high-quality systematic reviews. Screening prioritisation, or in other words, ranking the set of studies, enables downstream activities of a systematic review to begin in parallel. We investigate a method that exploits seed studies – potentially relevant studies used to seed the query formulation process – for screening prioritisation. Our investigation aims to reproduce this method to determine if it is generalisable on recently published datasets and determine the impact of using multiple seed studies on effectiveness. We show that while we could reproduce the original methods, we could not replicate their results exactly. However, we believe this is due to minor differences in document pre-processing, not deficiencies with the original methodology. Our results also indicate that our reproduced screening prioritisation method, (1) is generalisable across datasets of similar and different topicality compared to the original implementation, (2) that when using multiple seed studies, the effectiveness of the method increases using our techniques to enable this, (3) and that the use of multiple seed studies produces more stable rankings compared to single seed studies. Finally, we make our implementation and results publicly available at the following URL: &lt;a href="https://github.com/ielab/sdr" target="_blank" rel="noopener">https://github.com/ielab/sdr&lt;/a>&lt;/p></description></item><item><title>Query Automation for Systematic Reviews</title><link>https://scells.me/publication/phd_thesis/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://scells.me/publication/phd_thesis/</guid><description>&lt;p>This thesis explores and investigates query automation methods and tools to support more effective study retrieval for the creation of medical systematic reviews. A systematic review is a synthesis of very high-quality medical studies with a stringent protocol to guide creation with minimal bias and error. Modern medicine practice and healthcare policies rely heavily on systematic reviews in order to make decisions on the best available evidence.&lt;/p>
&lt;p>Systematic review creation is often highly time-intensive and costly. Thus there has been much attention to attempt to automate phases in the creation of a systematic review. In this context, the definition of automation is the encoding of manual tasks into computational procedures. Existing automation research has primarily focused on the phase concerned with the critical appraisal of studies. Prior research has attempted to automate this appraisal phase by, for example, simulating human appraisal, (re-)ranking the retrieved studies, and classifying relevant studies, among others. A major drawback of these approaches is that they all rely on the initial search&amp;rsquo;s retrieval effectiveness, which is contingent on the query&amp;rsquo;s quality. We follow a fresh, new direction of research by investigating methods for query automation.&lt;/p>
&lt;p>In investigating automatic methods for building effective queries for systematic review creation, this thesis develops three main areas of enquiry: (i) query formulation: the development of Boolean queries suitable for systematic review literature search, (ii) query refinement: the iterative process of improving the search effectiveness of a Boolean query suitable for systematic review literature search, and (iii) query exploitation: the application of methods to Boolean queries in order to improve systematic review literature search effectiveness. For each of these query automation areas, this thesis devises computational methods as well as practical tools.&lt;/p>
&lt;p>This thesis makes contributions across three main areas of enquiry. They comprise of: (i) computational adaptations of human query formulation methodologies to support the automatic development of complex Boolean queries for systematic review literature search, and practical tools built upon them, (ii) a framework for the generation and identification of more effective query variations to support the automatic refinement of high-quality Boolean queries used in systematic review literature search, as well as tools built upon them, and (iii) methods to exploit intrinsic characteristics of Boolean queries in order to improve the retrieval effectiveness of systematic review literature search. We found that for each of these inquiry areas, query automation was able to improve search effectiveness, reducing the amount of time necessary to create systematic reviews.&lt;/p></description></item><item><title>Big Brother: A Drop-In Website Interaction Logging Service</title><link>https://scells.me/publication/sigir2021_bigbro/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2021_bigbro/</guid><description>&lt;p>Fine-grained logging of interactions in user studies is important for studying user behaviour, among other reasons. However, in many research scenarios, the way interactions are logged are usually tied to a monolithic system. We present a generic, application-independent service for logging interactions in web-pages, specifically targetting user studies. Our service, Big Brother, can be dropped-in to existing user interfaces with almost no configuration required by researchers. Big Brother has already been used in several user studies to record interactions in a number of user study research scenarios, such as lab-based and crowdsourcing environments. We further demonstrate the ability for Big Brother to scale to very large user studies through benchmarking experiments. Big Brother also provides a number of additional tools for visualising and analysing interactions.&lt;/p>
&lt;p>Big Brother significantly lowers the barrier to entry for logging user interactions by providing a minimal but powerful, no configuration necessary, service for researchers and practitioners of user studies that can scale to thousands of concurrent sessions. We have made the source code and releases for Big Brother available for download at &lt;a href="https://github.com/hscells/bigbro" target="_blank" rel="noopener">https://github.com/hscells/bigbro&lt;/a>.&lt;/p></description></item><item><title>PECAN: A Platform for Searching Chat Conversations</title><link>https://scells.me/publication/sigir2021_pecan/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2021_pecan/</guid><description>&lt;p>Often, existing chat services that organisations and individuals use today provide a way to search through previously sent messages. However, many of these chat services provide far-limited search functionalities, typically exact matching on individual messages. In this paper, we introduce a new task for addressing this problem, called \textit{searching for conversations}, whereby the aim is to retrieve and rank groups of related messages given a search query. We promote this task by providing a platform for research and development called PECAN. Our platform provides all the necessary functionality researchers need to conduct experiments on searching for conversations. Our system is also generic so as to support organisations and individuals who wish to search through their chat message archives.&lt;/p>
&lt;p>We release PECAN to the wider community as an Open Source project available for download at &lt;a href="https://github.com/ielab/pecan" target="_blank" rel="noopener">https://github.com/ielab/pecan&lt;/a>.&lt;/p></description></item><item><title>You Can Teach an Old Dog New Tricks - Rank Fusion applied to Coordination Level Matching for Ranking in Systematic Reviews</title><link>https://scells.me/publication/ecir2020_clf/</link><pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ecir2020_clf/</guid><description>&lt;p>Coordination level matching is a ranking method originally proposed to rank documents given Boolean queries that is now several decades old. Rank fusion is a relatively recent method for combining runs from multiple systems into a single ranking, and has been shown to significantly improve the ranking. This paper presents a novel extension to coordination level matching, by applying rank fusion to each sub-clause of a Boolean query. We show that, for the tasks of systematic review screening prioritisation and stopping estimation, our method significantly outperforms the state-of-the-art learning to rank and bag-of-words-based systems for this domain. Our fully automatic, unsupervised method has (i) the potential for significant real-world cost savings (ii) does not rely on any intervention from the user, and (iii) is significantly better at ranking documents given only a Boolean query in the context of systematic reviews when compared to other approaches.&lt;/p></description></item><item><title>A Comparison of Automatic Boolean Query Formulation for Systematic Reviews</title><link>https://scells.me/publication/irj2020_comparison/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://scells.me/publication/irj2020_comparison/</guid><description>&lt;p>Systematic reviews are comprehensive literature reviews that target a highly focused research question. In the medical domain, complex Boolean queries are used to identify studies. To ensure comprehensiveness, all studies retrieved are screened for inclusion or exclusion in the review. Developing Boolean queries for this task requires the expertise of trained information specialists. However, even for these expert searchers, query formulation can be difficult and lengthy: especially when dealing with areas of medicine that they may not be knowledgeable about. To this end, two computational adaptations of methods information specialists use to formulate Boolean queries have been proposed in prior work.&lt;/p>
&lt;p>These adaptations can be used to assist information specialists by providing a good starting point for query development. However, a number of limitations with these computational methods have been raised, and a comparison between them has not been made. In this study, we address the limitations of previous work and evaluate the two.&lt;/p>
&lt;p>We found that, between the two computational adaptions, the objective method is more effective than the conceptual method for query formulation alone, however, the conceptual method provides a better starting point for manual query refinement. This work helps to inform those building search tools that assist with systematic review construction.&lt;/p></description></item><item><title>A Computational Approach for Objectively Derived Systematic Review Search Strategies</title><link>https://scells.me/publication/ecir2020_objective/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ecir2020_objective/</guid><description>&lt;p>Searching literature for a systematic review begins with a manually constructed search strategy by an expert information specialist. The typical process of constructing search strategies is often undocumented, ad-hoc, and subject to individual expertise, which may introduce bias in the systematic review. A new method for objectively deriving search strategies has arisen from information specialists attempting to address these shortcomings. However, this proposed method still presents a number of manual, ad-hoc interventions, and trial-and-error processes, potentially still introducing bias into systematic reviews. Moreover, this method has not been rigorously evaluated on a large set of systematic review cases, thus its generalisability is unknown. In this work, we present a computational adaptation of this proposed objective method. Our adaptation removes the human-in-the-loop processes involved in the initial steps of creating a search strategy for a systematic review; reducing bias due to human factors and increasing the objectivity of the originally proposed method. Our proposed computational adaptation further enables a formal and rigorous evaluation over a large set of systematic reviews. We find that our computational adaptation of the original objective method provides an effective starting point for information specialists to continue refining. We also identify a number of avenues for extending and improving our adaptation to further promote supporting information specialists.&lt;/p></description></item><item><title>Automatic Boolean Query Formulation for Systematic Review Literature Search</title><link>https://scells.me/publication/www2020_conceptual/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://scells.me/publication/www2020_conceptual/</guid><description>&lt;p>Formulating Boolean queries for systematic review literature search is a challenging task. Commonly, queries are formulated by information specialists using the protocol specified in the review and interactions with the research team. Information specialists have in-depth experience on how to formulate queries in this domain, but may not have in-depth knowledge about the reviews&amp;rsquo; topics. Query formulation requires a significant amount of time and effort, and is performed interactively; specialists repeatedly formulate queries, attempt to validate their results, and reformulate specific Boolean clauses. In this paper, we investigate the possibility of automatically formulating a Boolean query from the systematic review protocol. We propose a novel five-step approach to automatic query formulation, specific to Boolean queries in this domain, which approximates the process by which information specialists formulate queries. In this process, we use syntax parsing to derive the logical structure of high-level concepts in a query, automatically extract and map concepts to entities in order to perform entity expansion, and finally apply post-processing operations (such as stemming and search filters).&lt;/p>
&lt;p>Automatic query formulation for systematic review literature search has several benefits: (i) it can provide reviewers with an indication of the types of studies that will be retrieved, without the involvement of an information specialist, (ii) it can provide information specialists with an initial query to begin the formulation process, (iii) it can provide researchers that perform rapid reviews with a method to quickly perform searches.&lt;/p></description></item><item><title>Sampling Query Variations for Learning to Rank to Improve Automatic Boolean Query Generation in Systematic Reviews</title><link>https://scells.me/publication/www2020_sampling/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://scells.me/publication/www2020_sampling/</guid><description>&lt;p>Searching medical literature for synthesis in a systematic review is a complex and labour intensive task. In this context, expert searchers construct lengthy Boolean queries. The universe of possible query variations can be massive: a single query can be composed of hundreds of field-restricted search terms/phrases or ontological concepts, each grouped by a logical operator nested to depths of sometimes five or more levels deep. With the many choices about how to construct a query, it is difficult to both formulate and recognise effective queries. To address this challenge, automatic methods have recently been explored for generating and selecting effective Boolean query variations for systematic reviews. The limiting factor of these methods is that it is computationally infeasible to process all query variations for training the methods. To overcome this, we propose novel query variation sampling methods for training Learning to Rank models to rank queries. Our results show that query sampling methods do directly impact the ability of a Learning to Rank model to effectively identify good query variations. Thus, selecting appropriate query sampling methods is a key problem for the automatic reformulation of effective Boolean queries for systematic review literature search. We find that the best sampling strategies are those which balance the diversity of queries with the quantity of queries.&lt;/p></description></item><item><title>Automatic Boolean Query Refinement for Systematic Review Literature Search</title><link>https://scells.me/publication/cc2019_reformulation/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://scells.me/publication/cc2019_reformulation/</guid><description>&lt;iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/A1GtoNFWN0c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p>Background: Within the last decade the rise of digital publishing has become widespread, enabling publications to be edited and updated after-the-fact. In the medical domain, systematic reviews are one type of digital document that is often updated after initial publication. This is usually because new evidence has been discovered and must be re-synthesised into the existing review. A problem, however, is that the initial search strategy used to identify the originally relevant studies may not be sufficient in capturing new studies, or may capture too many irrelevant studies. This means that time and effort must be spent reformulating new or variant search strategies. While this problem may be particularly pronounced in “living systematic reviews”, the problem of finding all relevant studies while minimising irrelevant studies for typical systematic reviews is also difficult. This overarching problem signifies a gap to be filled with a system for automatic search strategy reformulation.&lt;/p>
&lt;p>Objectives: The development of an automatic, interactive search strategy reformulation tool that assists researchers in updating systematic reviews and to improve existing search strategies.&lt;/p>
&lt;p>Methods: The system proposed uses a recognised and effective theoretical framework which automatically generates search strategy reformulations and selects the most effective variation. In this work, a user interface (Figure 1) is developed with the goal to insert a human-in-the-loop to drive the selection of the most effective search strategy. This interface is capable of: (i) tracking the effectiveness of reformulations over time, allowing users to manage their reformulation history by backtracking and jumping to previous search strategies, (ii) evaluating the effectiveness of reformulations using standard Information Retrieval measures (e.g., precision, recall, F-measure), and domain-specific evaluation measures (e.g., Work Saved) by loading in a validation set of studies, and (iii) filtering out studies which have already been screened (also by loading separately) in order to only show new studies.&lt;/p>
&lt;p>Results: The theoretical framework for which the generation and selection of search strategy reformulations is based on is shown to significantly improve the effectiveness of existing queries. Queries are shown to increase in effectiveness upwards of 100-200% and beyond depending on the automatic selection process and evaluation measure.&lt;/p>
&lt;p>Conclusions: A human-in-the-loop for the selection of search strategy reformulation allows users to have fine-grained control over the reformulation process. Allowing humans to drive the selection process in this framework is a new and novel approach, which has not yet been attempted. Finally, automatically generating reformulations removes possible human bias and error, and reduces the time and effort required to update a review.&lt;/p>
&lt;p>Patient or healthcare consumer involvement: This has no direct involvement with patients or consumers, although improved efficiency of systematic review searches could be beneficial to both groups.&lt;/p></description></item><item><title>Automatic Boolean Query Refinement for Systematic Review Literature Search</title><link>https://scells.me/publication/www2019_boolean/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://scells.me/publication/www2019_boolean/</guid><description>&lt;p>In the medical domain, systematic reviews are a highly trustworthy evidence source used to inform clinical diagnosis and treatment, and governmental policy making. Systematic reviews must be complete in that &lt;em>all relevant&lt;/em> literature for the research question of the review must be synthesised in order to produce a recommendation. To identify the literature to screen for inclusion in systematic reviews, information specialists construct complex Boolean queries that capture the information needs defined by the research questions of the systemic review.
However, in the quest for total recall, these Boolean queries return many non relevant results.&lt;/p>
&lt;p>In this paper, we propose automatic methods for Boolean query refinement in the context of systematic review literature retrieval with the aim of alleviating this high recall-low precision problem. To do this, we build upon current literature and define additional semantic transformations for Boolean queries in the form of query expansion and reduction. Empirical evaluation is done on a set of real systematic review queries to show how our method performs in a realistic setting.
We found that query refinement strategies produced queries that were more effective than the original in terms of six information retrieval evaluation measures. In particular, queries were refined to increase precision, while maintaining, or even increasing, recall &amp;mdash; this, in turn, translates into both time and cost savings when creating laborious and expensive systematic reviews.&lt;/p></description></item><item><title>Visualising Systematic Review Search Strategies to Assist Information Specialists</title><link>https://scells.me/publication/cc2019_visualisation/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://scells.me/publication/cc2019_visualisation/</guid><description>&lt;iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/HPEQWCrMGWw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;p>Background: Searching for studies to include in systematic reviews involves the construction of complex search strategies. The effectiveness of these search strategies directly impacts the workload associated with conducting a systematic review. More efficient search strategies can reduce the time and resources required to screen studies therefore reducing the total time and cost of the review. Research in Information Retrieval (IR) shows that query visualisation improves the effectiveness of searching for information.&lt;/p>
&lt;p>Objectives: The development of a visualisation tool that assists information specialists in formulating more effective search strategies.&lt;/p>
&lt;p>Methods: The visualisation tool (QueryVis) is designed in collaboration with information specialists to cater to the needs of this user group. Currently in QueryVis, it is possible to visualise search strategies using the PubMed database. Searches can be submitted in Ovid MEDLINE format or PubMed format (by automatically translating the Ovid format to PubMed). QueryVis presents queries hierarchically (Figure 1) , showing the impact that each term has on the recall and precision of a search. Recall is shown by the number of studies retrieved from a validation set, loaded via PubMed IDs. Precision is shown by total number of studies found by each term. Search strategies are compared in terms of the overlap in search terms between two searches, the total number of keywords, the total number of each Boolean operator (i.e., AND, OR, NOT), the number of MeSH keywords (and how many are exploded), and the depth in the MeSH ontology in which the MeSH keywords appear (i.e., how broad MeSH keywords are). Furthermore, each search is evaluated in terms of effectiveness: using standard IR evaluation measures (i.e., precision, recall, F-measure) and evaluation measures specific to this domain (e.g., Work Saved), and in terms of efficiency: by recording the total time spent formulating, the number of studies retrieved, and the estimated cost of the search.&lt;/p>
&lt;p>Results: QueryVis has been tested experimentally in a pilot study. It decreased irrelevant studies by as much as 40% without losing relevant studies. A wider study is planned which aims to involve more participants and capture more data.&lt;/p>
&lt;p>Conclusions: Improving the query formulation stage can have a significant impact on the rest of the systematic review creation process. An extensive user study will follow using a well-known corpus of systematic reviews with approximately 10 participants to determine precisely what effect visualisation has on the search strategy formulation process. The study will use the aforementioned methods to compare the effect visualisation has on search by comparing search strategies formulated with and without visualisation. If accepted for publication, QueryVis will be demoed during the oral presentation.&lt;/p>
&lt;p>Patient or healthcare consumer involvement: This has no direct involvement with patients or consumers, although improved efficiency of systematic review searches could be beneficial to both groups.&lt;/p></description></item><item><title>An Information Retrieval Experiment Framework for Domain Specific Applications</title><link>https://scells.me/publication/sigir2018_framework/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2018_framework/</guid><description/></item><item><title>Generating Better Queries for Systematic Reviews</title><link>https://scells.me/publication/sigir2018_generating/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2018_generating/</guid><description>&lt;p>Systematic reviews form the cornerstone of evidence based medicine, aiming to answer complex medical questions based on all evidence currently available. Key to the effectiveness of a systematic review is an (often large) Boolean query used to search large publication repositories. These Boolean queries are carefully crafted by researchers and information specialists, and often reviewed by a panel of experts. However, little is known about the effectiveness of the Boolean queries at the time of formulation.&lt;/p>
&lt;p>In this paper we investigate whether a better Boolean query than that defined in the protocol of a systematic review, can be created, and we develop methods for the transformation of a given Boolean query into a more effective one. Our approach involves defining possible transformations of Boolean queries and their clauses. It also involves casting the problem of identifying a transformed query that is better than the original into: (i) a classification problem; and (ii) a learning to rank problem. Empirical experiments are conducted on a real set of systematic reviews. Analysis of results shows that query transformations that are better than the original queries do exist, and that our approaches are able to select more effective queries from the set of possible transformed queries so as to maximise different target effectiveness measures.&lt;/p></description></item><item><title>Query Variation Performance Prediction for Systematic Reviews</title><link>https://scells.me/publication/sigir2018_qvpp/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2018_qvpp/</guid><description/></item><item><title>searchrefiner: A Query Visualisation and Understanding Tool for Systematic Reviews</title><link>https://scells.me/publication/cikm2018_searchrefiner/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>https://scells.me/publication/cikm2018_searchrefiner/</guid><description>&lt;p>We present an open source tool, searchrefiner, for researchers that conduct medical systematic reviews to assist in formulating, visualising, and understanding Boolean queries. The searchrefiner web interface allows researchers to explore how Boolean queries retrieve citations in existing, popular query syntaxes used in systematic review literature search. The web interface allows researchers to perform tasks such as using validation citations to ensure queries are retrieving a minimum set of known relevant citations, and editing Boolean queries by dragging and dropping clauses in a structured editor. In addition, the tools provided by the searchrefiner interface allow researchers to visualise why the queries they formulate retrieve citations, and ways to understand how to refine queries into more effective ones. searchrefiner is targeted at both experts and novices, as a tool for query formulation and refinement, and as a tool for training users to search for literature to compile systematic reviews.&lt;/p>
&lt;p>The searchrefiner website located at &lt;a href="https://ielab.io/searchrefiner" target="_blank" rel="noopener">https://ielab.io/searchrefiner&lt;/a> contains information about how to download, install, and use the tool, as well as a link to an online hosted version for demonstration purposes.&lt;/p></description></item><item><title>A Test Collection for Evaluating Retrieval of Studies for Inclusion in Systematic Reviews</title><link>https://scells.me/publication/sigir2017_collection/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2017_collection/</guid><description>&lt;p>We introduce a test collection for evaluating the effectiveness of different methods used to retrieve research studies for inclusion in systematic reviews. Systematic reviews appraise and synthesise studies that meet specific inclusion criteria. Systematic reviews intended for a biomedical science audience use boolean queries with many, often complex, search clauses to retrieve studies; these are then manually screened to determine eligibility for inclusion in the review. This process is expensive and time consuming. The development of systems that improve retrieval effectiveness will have an immediate impact by reducing the complexity and resources required for this process. Our test collection consists of approximately 26 million research studies extracted from the freely available MEDLINE database, 94 review (query) topics extracted from Cochrane systematic reviews, and corresponding relevance assessments. Tasks for which the collection can be used for information retrieval system evaluation are described and the use of the collection to evaluate common baselines within one such task is demonstrated. See the links to this side of this page for links to the paper and to download the collection.&lt;/p></description></item><item><title>Integrating the Framing of Clinical Questions via PICO into the Retrieval of Medical Literature for Systematic Reviews</title><link>https://scells.me/publication/cikm2017_pico/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://scells.me/publication/cikm2017_pico/</guid><description/></item><item><title>QUT ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning To Rank</title><link>https://scells.me/publication/clef2017_tar/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://scells.me/publication/clef2017_tar/</guid><description/></item><item><title>Reducing Workload of Systematic Review Searching and Screening Processes</title><link>https://scells.me/publication/fdia2017_essir/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://scells.me/publication/fdia2017_essir/</guid><description/></item><item><title>Investigating Methods Of Annotating Lifelogs For Use In Search</title><link>https://scells.me/publication/honours_thesis/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://scells.me/publication/honours_thesis/</guid><description/></item><item><title>QUT at the NTCIR Lifelog Semantic Access Task</title><link>https://scells.me/publication/ntcir2016_lifelog/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ntcir2016_lifelog/</guid><description/></item></channel></rss>