<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shahbaz Syed | Harry Scells</title><link>/authors/shahbaz-syed/</link><atom:link href="/authors/shahbaz-syed/index.xml" rel="self" type="application/rss+xml"/><description>Shahbaz Syed</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 11 Jul 2024 00:00:00 +0000</lastBuildDate><image><url>/media/icon_hu33ac2647be7de12c09d025d9374b4e08_2029_512x512_fill_lanczos_center_3.png</url><title>Shahbaz Syed</title><link>/authors/shahbaz-syed/</link></image><item><title>Evaluating Generative Ad Hoc Information Retrieval</title><link>/publication/sigir2024_evaluating/</link><pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate><guid>/publication/sigir2024_evaluating/</guid><description>&lt;p>Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the established evaluation methodology for ranking-based ad hoc retrieval is not suited for the reliable and reproducible evaluation of generated responses. To lay a foundation for developing new evaluation methods for generative retrieval systems, we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a new user model, and study its operationalization.&lt;/p></description></item></channel></rss>