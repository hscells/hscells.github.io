<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Shengyao Zhuang | Harry Scells</title><link>https://scells.me/authors/shengyao-zhuang/</link><atom:link href="https://scells.me/authors/shengyao-zhuang/index.xml" rel="self" type="application/rss+xml"/><description>Shengyao Zhuang</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 11 Apr 2024 00:00:00 +0000</lastBuildDate><image><url>https://scells.me/media/icon_hu33ac2647be7de12c09d025d9374b4e08_2029_512x512_fill_lanczos_center_3.png</url><title>Shengyao Zhuang</title><link>https://scells.me/authors/shengyao-zhuang/</link></image><item><title>Set-Encoder: Permutation-Invariant Inter-Passage Attention for Listwise Passage Re-Ranking with Cross-Encoders</title><link>https://scells.me/publication/arxiv2024_setencoder/</link><pubDate>Thu, 11 Apr 2024 00:00:00 +0000</pubDate><guid>https://scells.me/publication/arxiv2024_setencoder/</guid><description>&lt;p>Cross-encoders are effective passage re-rankers. But when re-ranking multiple passages at once, existing cross-encoders inefficiently optimize the output ranking over several input permutations, as their passage interactions are not permutation-invariant. Moreover, their high memory footprint constrains the number of passages during listwise training. To tackle these issues, we propose the Set-Encoder, a new cross-encoder architecture that (1) introduces inter-passage attention with parallel passage processing to ensure permutation invariance between input passages, and that (2) uses fused-attention kernels to enable training with more passages at a time. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is more effective than previous cross-encoders with a similar number of parameters. Compared to larger models, the Set-Encoder is more efficient and either on par or even more effective.&lt;/p></description></item><item><title>Zero-shot Generative Large Language Models for Systematic Review Screening Automation</title><link>https://scells.me/publication/ecir2024_zero/</link><pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ecir2024_zero/</guid><description>&lt;p>Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models (LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.&lt;/p></description></item><item><title>Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of Information Retrieval Models</title><link>https://scells.me/publication/ictir2023_c02/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><guid>https://scells.me/publication/ictir2023_c02/</guid><description>&lt;p>As in other fields of artificial intelligence, the information retrieval community has grown interested in investigating the power consumption associated with neural models, particularly models of search. This interest has become particularly relevant as the energy consumption of information retrieval models has risen with new neural models based on large language models, leading to an associated increase of CO2 emissions, albeit relatively low compared to fields such as natural language processing. Consequently, researchers have started exploring the development of a green agenda for sustainable information retrieval research and operation. Previous work, however, has primarily considered energy consumption and associated CO2 emissions alone. In this paper, we seek to draw the information retrieval community&amp;rsquo;s attention to the overlooked aspect of water consumption related to these powerful models. We supplement previous energy consumption estimates with corresponding water consumption estimates, considering both off-site water consumption (required for operating and cooling energy production systems such as carbon and nuclear power plants) and on-site consumption (for cooling the data centres where models are trained and operated). By incorporating water consumption alongside energy consumption and CO2 emissions, we offer a more comprehensive understanding of the environmental impact of information retrieval research and operation.&lt;/p></description></item><item><title>Reduce, Reuse, Recycle: Green Information Retrieval Research</title><link>https://scells.me/publication/sigir2022_greenir/</link><pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate><guid>https://scells.me/publication/sigir2022_greenir/</guid><description>&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Awarded Best Paper Honorable Mention!&lt;/strong>
&lt;/div>
&lt;/div>
&lt;p>Recent advances in Information Retrieval utilise energy-intensive hardware to produce state-of-the-art results. In areas of research highly related to Information Retrieval, such as Natural Language Processing and Machine Learning, there have been efforts to quantify and reduce the power and emissions produced by methods that depend on such hardware. Research that is conscious of the environmental impacts of its experimentation and takes steps to mitigate some of these impacts is considered &amp;lsquo;Green&amp;rsquo;. Given the continuous demand for more data and power-hungry techniques, Green research is likely to become more important within the broader research community. Therefore, within the Information Retrieval community, the consequences of non-Green (in other words, Red) research should at least be considered and acknowledged. As such, the aims of this perspective paper are fourfold: (1) to review the Green literature not only for Information Retrieval but also for related domains in order to identify transferable Green techniques; (2) to provide measures for quantifying the power usage and emissions of Information Retrieval research; (3) to report the power usage and emission impacts for various current IR methods; and (4) to provide a framework to guide Green Information Retrieval research, taking inspiration from &amp;lsquo;reduce, reuse, recycle&amp;rsquo; waste management campaigns, including salient examples from the literature that implement these concepts.&lt;/p></description></item></channel></rss>