[{"authors":["admin"],"categories":null,"content":"I am currently completing my PhD under the supervision of Guido Zuccon at The University of Queensland in Australia.\nI am researching how to improve medical systematic review creation with Information Retrieval. My research focuses on developing methods to assist information specialists create more effective queries in shorter periods of time. To do this I am investigating automatic methods of query formulation, and query visualisation and understandability tools.\nOn this page please find a list of my publications with links to project pages, where relevant.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently completing my PhD under the supervision of Guido Zuccon at The University of Queensland in Australia.\nI am researching how to improve medical systematic review creation with Information Retrieval. My research focuses on developing methods to assist information specialists create more effective queries in shorter periods of time. To do this I am investigating automatic methods of query formulation, and query visualisation and understandability tools.\nOn this page please find a list of my publications with links to project pages, where relevant.","tags":null,"title":"Harry Scells","type":"authors"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman","Justin Clark"],"categories":null,"content":"Searching literature for a systematic review begins with a manually constructed search strategy by an expert information specialist. The typical process of constructing search strategies is often undocumented, ad-hoc, and subject to individual expertise, which may introduce bias in the systematic review. A new method for objectively deriving search strategies has arisen from information specialists attempting to address these shortcomings. However, this proposed method still presents a number of manual, ad-hoc interventions, and trial-and-error processes, potentially still introducing bias into systematic reviews. Moreover, this method has not been rigorously evaluated on a large set of systematic review cases, thus its generalisability is unknown. In this work, we present a computational adaptation of this proposed objective method. Our adaptation removes the human-in-the-loop processes involved in the initial steps of creating a search strategy for a systematic review; reducing bias due to human factors and increasing the objectivity of the originally proposed method. Our proposed computational adaptation further enables a formal and rigorous evaluation over a large set of systematic reviews. We find that our computational adaptation of the original objective method provides an effective starting point for information specialists to continue refining. We also identify a number of avenues for extending and improving our adaptation to further promote supporting information specialists.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"cbfadb57d3064fcd22eddb319e25c1ab","permalink":"/publication/ecir2020_objective/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/ecir2020_objective/","section":"publication","summary":"Searching literature for a systematic review begins with a manually constructed search strategy by an expert information specialist. The typical process of constructing search strategies is often undocumented, ad-hoc, and subject to individual expertise, which may introduce bias in the systematic review. A new method for objectively deriving search strategies has arisen from information specialists attempting to address these shortcomings. However, this proposed method still presents a number of manual, ad-hoc interventions, and trial-and-error processes, potentially still introducing bias into systematic reviews.","tags":null,"title":"A Computational Approach for Objectively Derived Systematic Review Search Strategies","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman","Justin Clark"],"categories":null,"content":"Formulating Boolean queries for systematic review literature search is a challenging task. Commonly, queries are formulated by information specialists using the protocol specified in the review and interactions with the research team. Information specialists have in-depth experience on how to formulate queries in this domain, but may not have in-depth knowledge about the reviews\u0026rsquo; topics. Query formulation requires a significant amount of time and effort, and is performed interactively; specialists repeatedly formulate queries, attempt to validate their results, and reformulate specific Boolean clauses. In this paper, we investigate the possibility of automatically formulating a Boolean query from the systematic review protocol. We propose a novel five-step approach to automatic query formulation, specific to Boolean queries in this domain, which approximates the process by which information specialists formulate queries. In this process, we use syntax parsing to derive the logical structure of high-level concepts in a query, automatically extract and map concepts to entities in order to perform entity expansion, and finally apply post-processing operations (such as stemming and search filters).\nAutomatic query formulation for systematic review literature search has several benefits: (i) it can provide reviewers with an indication of the types of studies that will be retrieved, without the involvement of an information specialist, (ii) it can provide information specialists with an initial query to begin the formulation process, (iii) it can provide researchers that perform rapid reviews with a method to quickly perform searches.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"0e97de998529bbe34eff2812b8637755","permalink":"/publication/www2020_conceptual/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/www2020_conceptual/","section":"publication","summary":"Formulating Boolean queries for systematic review literature search is a challenging task. Commonly, queries are formulated by information specialists using the protocol specified in the review and interactions with the research team. Information specialists have in-depth experience on how to formulate queries in this domain, but may not have in-depth knowledge about the reviews\u0026rsquo; topics. Query formulation requires a significant amount of time and effort, and is performed interactively; specialists repeatedly formulate queries, attempt to validate their results, and reformulate specific Boolean clauses.","tags":null,"title":"Automatic Boolean Query Formulation for Systematic Review Literature Search","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Mohamed A. Sharaf","Bevan Koopman"],"categories":null,"content":"Searching medical literature for synthesis in a systematic review is a complex and labour intensive task. In this context, expert searchers construct lengthy Boolean queries. The universe of possible query variations can be massive: a single query can be composed of hundreds of field-restricted search terms/phrases or ontological concepts, each grouped by a logical operator nested to depths of sometimes five or more levels deep. With the many choices about how to construct a query, it is difficult to both formulate and recognise effective queries. To address this challenge, automatic methods have recently been explored for generating and selecting effective Boolean query variations for systematic reviews. The limiting factor of these methods is that it is computationally infeasible to process all query variations for training the methods. To overcome this, we propose novel query variation sampling methods for training Learning to Rank models to rank queries. Our results show that query sampling methods do directly impact the ability of a Learning to Rank model to effectively identify good query variations. Thus, selecting appropriate query sampling methods is a key problem for the automatic reformulation of effective Boolean queries for systematic review literature search. We find that the best sampling strategies are those which balance the diversity of queries with the quantity of queries.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f90e12ce9cb8cfb6046f6828322b3f90","permalink":"/publication/www2020_sampling/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/www2020_sampling/","section":"publication","summary":"Searching medical literature for synthesis in a systematic review is a complex and labour intensive task. In this context, expert searchers construct lengthy Boolean queries. The universe of possible query variations can be massive: a single query can be composed of hundreds of field-restricted search terms/phrases or ontological concepts, each grouped by a logical operator nested to depths of sometimes five or more levels deep. With the many choices about how to construct a query, it is difficult to both formulate and recognise effective queries.","tags":null,"title":"Sampling Query Variations for Learning to Rank to Improve Automatic Boolean Query Generation in Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman"],"categories":null,"content":"Coordination level matching is a ranking method originally proposed to rank documents given Boolean queries that is now several decades old. Rank fusion is a relatively recent method for combining runs from multiple systems into a single ranking, and has been shown to significantly improve the ranking. This paper presents a novel extension to coordination level matching, by applying rank fusion to each sub-clause of a Boolean query. We show that, for the tasks of systematic review screening prioritisation and stopping estimation, our method significantly outperforms the state-of-the-art learning to rank and bag-of-words-based systems for this domain. Our fully automatic, unsupervised method has (i) the potential for significant real-world cost savings (ii) does not rely on any intervention from the user, and (iii) is significantly better at ranking documents given only a Boolean query in the context of systematic reviews when compared to other approaches.\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2849360865f27b6df6b46edf619523a8","permalink":"/publication/ecir2020_clf/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/ecir2020_clf/","section":"publication","summary":"Coordination level matching is a ranking method originally proposed to rank documents given Boolean queries that is now several decades old. Rank fusion is a relatively recent method for combining runs from multiple systems into a single ranking, and has been shown to significantly improve the ranking. This paper presents a novel extension to coordination level matching, by applying rank fusion to each sub-clause of a Boolean query. We show that, for the tasks of systematic review screening prioritisation and stopping estimation, our method significantly outperforms the state-of-the-art learning to rank and bag-of-words-based systems for this domain.","tags":null,"title":"You Can Teach an Old Dog New Tricks - Rank Fusion applied to Coordination Level Matching for Ranking in Systematic Reviews","type":"publication"},{"authors":null,"categories":null,"content":" MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i Ranking Documents The traditional task of ranking in Information Retrieval is to rank documents given a query. That is, given a set of documents $D$ and a query $q$, assign a score to each document $f(q, d_i)$. Learning to Rank (ltr) aims to model the scoring function by learning features about query-document pairs. To do this, a feature vector $x$ is created from each query-document pair using a feature function $\\phi(q,d_i)$.1 The ranking model assigns scores to according to the feature vector, equivalent to scoring the document-query pair: $f(x) = f(q, d_i)$\nRanking Queries This idea of ranking query-document pairs using a feature function can also be transposed to ranking queries (for example, ranking variations of query expansions). The feature function now becomes $\\phi(q)$, and the ranking model can now assign scores to queries.\n  http://times.cs.uiuc.edu/course/598f14/l2r.pdf \u0026#x21a9;\u0026#xfe0e;\n   ","date":1553817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553817600,"objectID":"4d417b32f6f4fc08002e61728334a84f","permalink":"/post/learning-to-rank/","publishdate":"2019-03-29T00:00:00Z","relpermalink":"/post/learning-to-rank/","section":"post","summary":"MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i Ranking Documents The traditional task of ranking in Information Retrieval is to rank documents given a query.","tags":null,"title":"Learning to Rank Queries","type":"post"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman","Justin Clark"],"categories":null,"content":" Background: Within the last decade the rise of digital publishing has become widespread, enabling publications to be edited and updated after-the-fact. In the medical domain, systematic reviews are one type of digital document that is often updated after initial publication. This is usually because new evidence has been discovered and must be re-synthesised into the existing review. A problem, however, is that the initial search strategy used to identify the originally relevant studies may not be sufficient in capturing new studies, or may capture too many irrelevant studies. This means that time and effort must be spent reformulating new or variant search strategies. While this problem may be particularly pronounced in “living systematic reviews”, the problem of finding all relevant studies while minimising irrelevant studies for typical systematic reviews is also difficult. This overarching problem signifies a gap to be filled with a system for automatic search strategy reformulation.\nObjectives: The development of an automatic, interactive search strategy reformulation tool that assists researchers in updating systematic reviews and to improve existing search strategies.\nMethods: The system proposed uses a recognised and effective theoretical framework which automatically generates search strategy reformulations and selects the most effective variation. In this work, a user interface (Figure 1) is developed with the goal to insert a human-in-the-loop to drive the selection of the most effective search strategy. This interface is capable of: (i) tracking the effectiveness of reformulations over time, allowing users to manage their reformulation history by backtracking and jumping to previous search strategies, (ii) evaluating the effectiveness of reformulations using standard Information Retrieval measures (e.g., precision, recall, F-measure), and domain-specific evaluation measures (e.g., Work Saved) by loading in a validation set of studies, and (iii) filtering out studies which have already been screened (also by loading separately) in order to only show new studies.\nResults: The theoretical framework for which the generation and selection of search strategy reformulations is based on is shown to significantly improve the effectiveness of existing queries. Queries are shown to increase in effectiveness upwards of 100-200% and beyond depending on the automatic selection process and evaluation measure.\nConclusions: A human-in-the-loop for the selection of search strategy reformulation allows users to have fine-grained control over the reformulation process. Allowing humans to drive the selection process in this framework is a new and novel approach, which has not yet been attempted. Finally, automatically generating reformulations removes possible human bias and error, and reduces the time and effort required to update a review.\nPatient or healthcare consumer involvement: This has no direct involvement with patients or consumers, although improved efficiency of systematic review searches could be beneficial to both groups.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"293ce5795647970ec9c3690a561067ae","permalink":"/publication/cc2019_reformulation/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/cc2019_reformulation/","section":"publication","summary":"Background: Within the last decade the rise of digital publishing has become widespread, enabling publications to be edited and updated after-the-fact. In the medical domain, systematic reviews are one type of digital document that is often updated after initial publication. This is usually because new evidence has been discovered and must be re-synthesised into the existing review. A problem, however, is that the initial search strategy used to identify the originally relevant studies may not be sufficient in capturing new studies, or may capture too many irrelevant studies.","tags":null,"title":"Automatic Boolean Query Refinement for Systematic Review Literature Search","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman"],"categories":null,"content":"In the medical domain, systematic reviews are a highly trustworthy evidence source used to inform clinical diagnosis and treatment, and governmental policy making. Systematic reviews must be complete in that all relevant literature for the research question of the review must be synthesised in order to produce a recommendation. To identify the literature to screen for inclusion in systematic reviews, information specialists construct complex Boolean queries that capture the information needs defined by the research questions of the systemic review. However, in the quest for total recall, these Boolean queries return many non relevant results.\nIn this paper, we propose automatic methods for Boolean query refinement in the context of systematic review literature retrieval with the aim of alleviating this high recall-low precision problem. To do this, we build upon current literature and define additional semantic transformations for Boolean queries in the form of query expansion and reduction. Empirical evaluation is done on a set of real systematic review queries to show how our method performs in a realistic setting. We found that query refinement strategies produced queries that were more effective than the original in terms of six information retrieval evaluation measures. In particular, queries were refined to increase precision, while maintaining, or even increasing, recall \u0026mdash; this, in turn, translates into both time and cost savings when creating laborious and expensive systematic reviews.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"b5ab0247a1dc7bca6c186ce6fb0d0908","permalink":"/publication/www2019_boolean/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/www2019_boolean/","section":"publication","summary":"In the medical domain, systematic reviews are a highly trustworthy evidence source used to inform clinical diagnosis and treatment, and governmental policy making. Systematic reviews must be complete in that all relevant literature for the research question of the review must be synthesised in order to produce a recommendation. To identify the literature to screen for inclusion in systematic reviews, information specialists construct complex Boolean queries that capture the information needs defined by the research questions of the systemic review.","tags":null,"title":"Automatic Boolean Query Refinement for Systematic Review Literature Search","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman","Justin Clark"],"categories":null,"content":" Background: Searching for studies to include in systematic reviews involves the construction of complex search strategies. The effectiveness of these search strategies directly impacts the workload associated with conducting a systematic review. More efficient search strategies can reduce the time and resources required to screen studies therefore reducing the total time and cost of the review. Research in Information Retrieval (IR) shows that query visualisation improves the effectiveness of searching for information.\nObjectives: The development of a visualisation tool that assists information specialists in formulating more effective search strategies.\nMethods: The visualisation tool (QueryVis) is designed in collaboration with information specialists to cater to the needs of this user group. Currently in QueryVis, it is possible to visualise search strategies using the PubMed database. Searches can be submitted in Ovid MEDLINE format or PubMed format (by automatically translating the Ovid format to PubMed). QueryVis presents queries hierarchically (Figure 1) , showing the impact that each term has on the recall and precision of a search. Recall is shown by the number of studies retrieved from a validation set, loaded via PubMed IDs. Precision is shown by total number of studies found by each term. Search strategies are compared in terms of the overlap in search terms between two searches, the total number of keywords, the total number of each Boolean operator (i.e., AND, OR, NOT), the number of MeSH keywords (and how many are exploded), and the depth in the MeSH ontology in which the MeSH keywords appear (i.e., how broad MeSH keywords are). Furthermore, each search is evaluated in terms of effectiveness: using standard IR evaluation measures (i.e., precision, recall, F-measure) and evaluation measures specific to this domain (e.g., Work Saved), and in terms of efficiency: by recording the total time spent formulating, the number of studies retrieved, and the estimated cost of the search.\nResults: QueryVis has been tested experimentally in a pilot study. It decreased irrelevant studies by as much as 40% without losing relevant studies. A wider study is planned which aims to involve more participants and capture more data.\nConclusions: Improving the query formulation stage can have a significant impact on the rest of the systematic review creation process. An extensive user study will follow using a well-known corpus of systematic reviews with approximately 10 participants to determine precisely what effect visualisation has on the search strategy formulation process. The study will use the aforementioned methods to compare the effect visualisation has on search by comparing search strategies formulated with and without visualisation. If accepted for publication, QueryVis will be demoed during the oral presentation.\nPatient or healthcare consumer involvement: This has no direct involvement with patients or consumers, although improved efficiency of systematic review searches could be beneficial to both groups.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"200a87fa116bd3cde5243b5c6f4834d8","permalink":"/publication/cc2019_visualisation/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/cc2019_visualisation/","section":"publication","summary":"Background: Searching for studies to include in systematic reviews involves the construction of complex search strategies. The effectiveness of these search strategies directly impacts the workload associated with conducting a systematic review. More efficient search strategies can reduce the time and resources required to screen studies therefore reducing the total time and cost of the review. Research in Information Retrieval (IR) shows that query visualisation improves the effectiveness of searching for information.","tags":null,"title":"Visualising Systematic Review Search Strategies to Assist Information Specialists","type":"publication"},{"authors":["Harrisen Scells","Daniel Locke","Guido Zuccon"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"fea346ebc9abe69f4c000de4d4e682f0","permalink":"/publication/sigir2018_framework/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/sigir2018_framework/","section":"publication","summary":"","tags":null,"title":"An Information Retrieval Experiment Framework for Domain Specific Applications","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon"],"categories":null,"content":"Systematic reviews form the cornerstone of evidence based medicine, aiming to answer complex medical questions based on all evidence currently available. Key to the effectiveness of a systematic review is an (often large) Boolean query used to search large publication repositories. These Boolean queries are carefully crafted by researchers and information specialists, and often reviewed by a panel of experts. However, little is known about the effectiveness of the Boolean queries at the time of formulation.\nIn this paper we investigate whether a better Boolean query than that defined in the protocol of a systematic review, can be created, and we develop methods for the transformation of a given Boolean query into a more effective one. Our approach involves defining possible transformations of Boolean queries and their clauses. It also involves casting the problem of identifying a transformed query that is better than the original into: (i) a classification problem; and (ii) a learning to rank problem. Empirical experiments are conducted on a real set of systematic reviews. Analysis of results shows that query transformations that are better than the original queries do exist, and that our approaches are able to select more effective queries from the set of possible transformed queries so as to maximise different target effectiveness measures.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"a947b6d3f3c6fb81f3f69baa46440b25","permalink":"/publication/sigir2018_generating/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/sigir2018_generating/","section":"publication","summary":"Systematic reviews form the cornerstone of evidence based medicine, aiming to answer complex medical questions based on all evidence currently available. Key to the effectiveness of a systematic review is an (often large) Boolean query used to search large publication repositories. These Boolean queries are carefully crafted by researchers and information specialists, and often reviewed by a panel of experts. However, little is known about the effectiveness of the Boolean queries at the time of formulation.","tags":null,"title":"Generating Better Queries for Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Leif Azzopardo","Guido Zuccon","Bevan Koopman"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"dd453e0e1ea6994c202341721bf20891","permalink":"/publication/sigir2018_qvpp/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/sigir2018_qvpp/","section":"publication","summary":"","tags":null,"title":"Query Variation Performance Prediction for Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon"],"categories":null,"content":"We present an open source tool, searchrefiner, for researchers that conduct medical systematic reviews to assist in formulating, visualising, and understanding Boolean queries. The searchrefiner web interface allows researchers to explore how Boolean queries retrieve citations in existing, popular query syntaxes used in systematic review literature search. The web interface allows researchers to perform tasks such as using validation citations to ensure queries are retrieving a minimum set of known relevant citations, and editing Boolean queries by dragging and dropping clauses in a structured editor. In addition, the tools provided by the searchrefiner interface allow researchers to visualise why the queries they formulate retrieve citations, and ways to understand how to refine queries into more effective ones. searchrefiner is targeted at both experts and novices, as a tool for query formulation and refinement, and as a tool for training users to search for literature to compile systematic reviews.\nThe searchrefiner website located at https://ielab.io/searchrefiner contains information about how to download, install, and use the tool, as well as a link to an online hosted version for demonstration purposes.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"71672abfbd12c9b62c64ba2cd97d8e80","permalink":"/publication/cikm2018_searchrefiner/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/cikm2018_searchrefiner/","section":"publication","summary":"We present an open source tool, searchrefiner, for researchers that conduct medical systematic reviews to assist in formulating, visualising, and understanding Boolean queries. The searchrefiner web interface allows researchers to explore how Boolean queries retrieve citations in existing, popular query syntaxes used in systematic review literature search. The web interface allows researchers to perform tasks such as using validation citations to ensure queries are retrieving a minimum set of known relevant citations, and editing Boolean queries by dragging and dropping clauses in a structured editor.","tags":null,"title":"searchrefiner: A Query Visualisation and Understanding Tool for Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Bevan Koopman","Anthony Deacon","Leif Azzopardi","Shlomo Geva"],"categories":null,"content":"We introduce a test collection for evaluating the effectiveness of different methods used to retrieve research studies for inclusion in systematic reviews. Systematic reviews appraise and synthesise studies that meet specific inclusion criteria. Systematic reviews intended for a biomedical science audience use boolean queries with many, often complex, search clauses to retrieve studies; these are then manually screened to determine eligibility for inclusion in the review. This process is expensive and time consuming. The development of systems that improve retrieval effectiveness will have an immediate impact by reducing the complexity and resources required for this process. Our test collection consists of approximately 26 million research studies extracted from the freely available MEDLINE database, 94 review (query) topics extracted from Cochrane systematic reviews, and corresponding relevance assessments. Tasks for which the collection can be used for information retrieval system evaluation are described and the use of the collection to evaluate common baselines within one such task is demonstrated. See the links to this side of this page for links to the paper and to download the collection.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6c0e3aaf8f594b1eb00f4c9c2a0e6d18","permalink":"/publication/sigir2017_collection/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/sigir2017_collection/","section":"publication","summary":"We introduce a test collection for evaluating the effectiveness of different methods used to retrieve research studies for inclusion in systematic reviews. Systematic reviews appraise and synthesise studies that meet specific inclusion criteria. Systematic reviews intended for a biomedical science audience use boolean queries with many, often complex, search clauses to retrieve studies; these are then manually screened to determine eligibility for inclusion in the review. This process is expensive and time consuming.","tags":null,"title":"A Test Collection for Evaluating Retrieval of Studies for Inclusion in Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Anthony Deacon","Bevan Koopman"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"deb1d81846ccf16b85206880807430df","permalink":"/publication/cikm2017_pico/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/cikm2017_pico/","section":"publication","summary":"","tags":null,"title":"Integrating the Framing of Clinical Questions via PICO into the Retrieval of Medical Literature for Systematic Reviews","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Anthony Deacon","Bevan Koopman"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"cd3effd36f4d48b7b9b4c4c098581960","permalink":"/publication/clef2017_tar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/clef2017_tar/","section":"publication","summary":"","tags":null,"title":"QUT ielab at CLEF eHealth 2017 Technology Assisted Reviews Track: Initial Experiments with Learning To Rank","type":"publication"},{"authors":["Harrisen Scells"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"49b8e6178b2cc93b148e1217c530ed9e","permalink":"/publication/fdia2017_essir/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/fdia2017_essir/","section":"publication","summary":"","tags":null,"title":"Reducing Workload of Systematic Review Searching and Screening Processes","type":"publication"},{"authors":["Harrisen Scells"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ec7b0da516eb00a93e6cf9d104c6151c","permalink":"/publication/honours_thesis/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/honours_thesis/","section":"publication","summary":"","tags":null,"title":"Investigating Methods Of Annotating Lifelogs For Use In Search","type":"publication"},{"authors":["Harrisen Scells","Guido Zuccon","Kirsty Kitto"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"33375a45bb0c59c630907a831b587a0d","permalink":"/publication/ntcir2016_lifelog/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/ntcir2016_lifelog/","section":"publication","summary":"","tags":null,"title":"QUT at the NTCIR Lifelog Semantic Access Task","type":"publication"}]